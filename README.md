# DecisionTreesSeminar

Добро пожаловать в **DecisionTreesSeminar** — репозиторий, содержащий Jupyter Notebook (`ML-4-1.ipynb`), посвященный реализации функции `find_best_split` для поиска оптимального разбиения подмножества обучающей выборки в задачах регрессии и классификации. Проект тестируется на датасете **Boston Housing** из библиотеки `sklearn` для задачи регрессии.

## Обзор

Репозиторий включает Jupyter Notebook (`ML-4-1.ipynb`), который охватывает следующие темы:
- Реализация функции `find_best_split`:
  - Для регрессии: использование **дисперсии** подвыборки в качестве критерия ошибки.
  - Для классификации: использование **критерия Джини** для оценки качества разбиения.
- Тестирование функции на датасете **Boston Housing** для регрессии.
- Визуализация зависимости критерия ошибки (отрицательной взвешенной дисперсии) от порогового значения признака (например, `CRIM`).

Ноутбук содержит код, комментарии и визуализацию результатов для анализа оптимального разбиения.

## Набор данных

- **Boston Housing**:
  - Датасет содержит информацию о ценах на жилье в пригородах Бостона (506 записей) и используется для задачи регрессии.
  - Признаки (13):
    - `CRIM`: Уровень преступности на душу населения.
    - `ZN`: Доля жилой земли, зонированной под участки более 25 000 кв. футов.
    - `INDUS`: Доля акров, занятых нежилыми предприятиями.
    - `CHAS`: Бинарный признак, указывающий на близость к реке Чарльз (1 — да, 0 — нет).
    - `NOX`: Концентрация оксидов азота.
    - `RM`: Среднее количество комнат в доме.
    - `AGE`: Доля домов, построенных до 1940 года.
    - `DIS`: Взвешенное расстояние до пяти центров занятости Бостона.
    - `RAD`: Индекс доступности радиальных магистралей.
    - `TAX`: Ставка налога на имущество.
    - `PTRATIO`: Соотношение учеников и учителей.
    - `B`: Доля афроамериканского населения (рассчитывается как 1000*(B_k - 0.63)^2, где B_k — доля).
    - `LSTAT`: Доля населения с низким социальным статусом.
  - Целевая переменная: `target` — медианная стоимость дома (в тысячах долларов).
  - Данные загружаются из `sklearn.datasets.load_boston` (в новых версиях `sklearn` заменён на `fetch_openml` или внешние источники).

## Требования

Для работы с ноутбуком необходимы следующие библиотеки Python:
- `numpy`
- `pandas`
- `matplotlib`
- `scikit-learn`

Установите их с помощью команды:
```bash
pip install numpy pandas matplotlib scikit-learn
```

## Структура репозитория

- `ML-4-1.ipynb`: Основной Jupyter Notebook с реализацией функции `find_best_split` и тестированием на датасете Boston.
- `README.md`: Этот файл с описанием проекта.

## Использование

1. Склонируйте репозиторий:
   ```bash
   git clone https://github.com/<your-username>/DecisionTreesSeminar.git
   ```
2. Перейдите в папку репозитория:
   ```bash
   cd DecisionTreesSeminar
   ```
3. Запустите Jupyter Notebook:
   ```bash
   jupyter notebook ML-4-1.ipynb
   ```
4. Следуйте инструкциям в ноутбуке для выполнения кода, анализа данных и визуализации результатов.

## Основные разделы ноутбука

1. **Загрузка и подготовка данных**:
   - Датасет загружается с использованием `sklearn.datasets.load_boston` (или альтернативного метода в новых версиях).
   - Данные преобразуются в `pandas.DataFrame` для удобства анализа:
     ```python
     from sklearn.datasets import load_boston
     boston = load_boston()
     df = pd.DataFrame(boston.data, columns=boston.feature_names)
     df['target'] = boston.target
     ```
   - Проверка структуры данных (`df.info()`): 506 записей, 14 столбцов (13 признаков + 1 целевая переменная).

2. **Реализация функции `find_best_split`**:
   - Функция принимает подмножество данных и определяет оптимальное разбиение по признаку и порогу.
   - Для регрессии:
     - Критерий: минимизация взвешенной дисперсии подвыборок:
       ```python
       variance = -(|R_l|/|R|) * var(R_l) - (|R_r|/|R|) * var(R_r)
       ```
       где `R_l` и `R_r` — левая и правая подвыборки, `var` — дисперсия.
     - Для каждого признака перебираются возможные пороговые значения, и выбирается разбиение с минимальной дисперсией.
   - Для классификации:
     - Критерий: максимизация критерия Джини:
       ```python
       Q(R) = -(|R_l|/|R|) * H(R_l) - (|R_r|/|R|) * H(R_r)
       ```
       где `H(R) = 1 - p_1^2 - p_0^2`, `p_1` и `p_0` — доли классов 1 и 0.
     - Для категориальных признаков: объекты с конкретным значением признака отправляются в левое поддерево, остальные — в правое.
   - Возвращает: индекс признака, пороговое значение и значение критерия.

3. **Тестирование на датасете Boston**:
   - Функция применяется к данным Boston для задачи регрессии.
   - Пример: поиск оптимального разбиения по признаку `CRIM` (уровень преступности).
   - Рассчитываются пороговые значения и соответствующие им значения критерия ошибки (отрицательная взвешенная дисперсия).

4. **Визуализация**:
   - Построение графика зависимости критерия ошибки от порогового значения для признака `CRIM`:
     ```python
     plt.figure(figsize=(10, 6))
     plt.plot(thresholds, variances, marker='o')
     plt.xlabel("Пороговое значение (CRIM)")
     plt.ylabel("Критерий ошибки (отрицательная взвешенная дисперсия)")
     plt.title("Зависимость критерия ошибки от порога для признака CRIM")
     plt.grid(True)
     plt.show()
     ```
   - График показывает, как критерий ошибки изменяется при различных порогах, помогая выбрать оптимальное разбиение.


## Результаты

- Функция `find_best_split` успешно находит оптимальное разбиение для задачи регрессии на датасете Boston.
- Для признака `CRIM` график показывает, как критерий ошибки (отрицательная взвешенная дисперсия) изменяется с порогом, что помогает выбрать лучшее разбиение.
- Реализация универсальна и может быть адаптирована для классификации с использованием критерия Джини.
